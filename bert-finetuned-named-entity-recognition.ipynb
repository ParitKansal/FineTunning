{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:26:40.576091Z","iopub.execute_input":"2025-02-05T17:26:40.576461Z","iopub.status.idle":"2025-02-05T17:26:55.180419Z","shell.execute_reply.started":"2025-02-05T17:26:40.576431Z","shell.execute_reply":"2025-02-05T17:26:55.179498Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers seqeval[gpu]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:26:55.181584Z","iopub.execute_input":"2025-02-05T17:26:55.182244Z","iopub.status.idle":"2025-02-05T17:26:59.482800Z","shell.execute_reply.started":"2025-02-05T17:26:55.182209Z","shell.execute_reply":"2025-02-05T17:26:59.481654Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: seqeval[gpu] in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:26:59.484798Z","iopub.execute_input":"2025-02-05T17:26:59.485040Z","iopub.status.idle":"2025-02-05T17:26:59.565341Z","shell.execute_reply.started":"2025-02-05T17:26:59.485019Z","shell.execute_reply":"2025-02-05T17:26:59.564642Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/ner-dataset/ner_datasetreference.csv\", encoding='unicode_escape')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:26:59.566792Z","iopub.execute_input":"2025-02-05T17:26:59.567048Z","iopub.status.idle":"2025-02-05T17:27:00.179912Z","shell.execute_reply.started":"2025-02-05T17:26:59.567028Z","shell.execute_reply":"2025-02-05T17:27:00.179055Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    Sentence #           Word  POS Tag\n0  Sentence: 1      Thousands  NNS   O\n1          NaN             of   IN   O\n2          NaN  demonstrators  NNS   O\n3          NaN           have  VBP   O\n4          NaN        marched  VBN   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>demonstrators</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>marched</td>\n      <td>VBN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"data['Tag'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:00.180794Z","iopub.execute_input":"2025-02-05T17:27:00.181125Z","iopub.status.idle":"2025-02-05T17:27:00.242702Z","shell.execute_reply.started":"2025-02-05T17:27:00.181099Z","shell.execute_reply":"2025-02-05T17:27:00.242019Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Tag\nO        887908\nB-geo     37644\nB-tim     20333\nB-org     20143\nI-per     17251\nB-per     16990\nI-org     16784\nB-gpe     15870\nI-geo      7414\nI-tim      6528\nB-art       402\nB-eve       308\nI-art       297\nI-eve       253\nB-nat       201\nI-gpe       198\nI-nat        51\nName: count, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data = data.ffill()\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:00.243826Z","iopub.execute_input":"2025-02-05T17:27:00.244194Z","iopub.status.idle":"2025-02-05T17:27:00.728459Z","shell.execute_reply.started":"2025-02-05T17:27:00.244159Z","shell.execute_reply":"2025-02-05T17:27:00.727647Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    Sentence #           Word  POS Tag\n0  Sentence: 1      Thousands  NNS   O\n1  Sentence: 1             of   IN   O\n2  Sentence: 1  demonstrators  NNS   O\n3  Sentence: 1           have  VBP   O\n4  Sentence: 1        marched  VBN   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 1</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 1</td>\n      <td>demonstrators</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 1</td>\n      <td>marched</td>\n      <td>VBN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data[\"sentence\"] = data.groupby(\"Sentence #\")[\"Word\"].transform(lambda x: ' | '.join(x))\ndata[\"word_labels\"] = data.groupby(\"Sentence #\")[\"Tag\"].transform(lambda x: ' | '.join(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:00.729299Z","iopub.execute_input":"2025-02-05T17:27:00.729635Z","iopub.status.idle":"2025-02-05T17:27:08.895030Z","shell.execute_reply.started":"2025-02-05T17:27:00.729605Z","shell.execute_reply":"2025-02-05T17:27:08.894296Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:08.897078Z","iopub.execute_input":"2025-02-05T17:27:08.897308Z","iopub.status.idle":"2025-02-05T17:27:08.906772Z","shell.execute_reply.started":"2025-02-05T17:27:08.897288Z","shell.execute_reply":"2025-02-05T17:27:08.905932Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    Sentence #           Word  POS Tag  \\\n0  Sentence: 1      Thousands  NNS   O   \n1  Sentence: 1             of   IN   O   \n2  Sentence: 1  demonstrators  NNS   O   \n3  Sentence: 1           have  VBP   O   \n4  Sentence: 1        marched  VBN   O   \n\n                                            sentence  \\\n0  Thousands | of | demonstrators | have | marche...   \n1  Thousands | of | demonstrators | have | marche...   \n2  Thousands | of | demonstrators | have | marche...   \n3  Thousands | of | demonstrators | have | marche...   \n4  Thousands | of | demonstrators | have | marche...   \n\n                                         word_labels  \n0  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n1  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n2  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n3  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n4  O | O | O | O | O | O | B-geo | O | O | O | O ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands</td>\n      <td>NNS</td>\n      <td>O</td>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 1</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 1</td>\n      <td>demonstrators</td>\n      <td>NNS</td>\n      <td>O</td>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>O</td>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 1</td>\n      <td>marched</td>\n      <td>VBN</td>\n      <td>O</td>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data['Tag'].unique()\nlabel2id = {}\nid2label = {}\ni = 0\nfor x in data['Tag'].unique():\n    label2id[x] = i\n    id2label[i] = x\n    i = i+1\nlabel2id, id2label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:30:12.224082Z","iopub.execute_input":"2025-02-05T17:30:12.224464Z","iopub.status.idle":"2025-02-05T17:30:12.325363Z","shell.execute_reply.started":"2025-02-05T17:30:12.224434Z","shell.execute_reply":"2025-02-05T17:30:12.324669Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"({'O': 0,\n  'B-geo': 1,\n  'B-gpe': 2,\n  'B-per': 3,\n  'I-geo': 4,\n  'B-org': 5,\n  'I-org': 6,\n  'B-tim': 7,\n  'B-art': 8,\n  'I-art': 9,\n  'I-per': 10,\n  'I-gpe': 11,\n  'I-tim': 12,\n  'B-nat': 13,\n  'B-eve': 14,\n  'I-eve': 15,\n  'I-nat': 16},\n {0: 'O',\n  1: 'B-geo',\n  2: 'B-gpe',\n  3: 'B-per',\n  4: 'I-geo',\n  5: 'B-org',\n  6: 'I-org',\n  7: 'B-tim',\n  8: 'B-art',\n  9: 'I-art',\n  10: 'I-per',\n  11: 'I-gpe',\n  12: 'I-tim',\n  13: 'B-nat',\n  14: 'B-eve',\n  15: 'I-eve',\n  16: 'I-nat'})"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"df = data.drop(columns=['Word', 'POS', 'Tag','Sentence #'])\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:09.011036Z","iopub.execute_input":"2025-02-05T17:27:09.011320Z","iopub.status.idle":"2025-02-05T17:27:09.038075Z","shell.execute_reply.started":"2025-02-05T17:27:09.011287Z","shell.execute_reply":"2025-02-05T17:27:09.037408Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                            sentence  \\\n0  Thousands | of | demonstrators | have | marche...   \n1  Thousands | of | demonstrators | have | marche...   \n2  Thousands | of | demonstrators | have | marche...   \n3  Thousands | of | demonstrators | have | marche...   \n4  Thousands | of | demonstrators | have | marche...   \n\n                                         word_labels  \n0  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n1  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n2  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n3  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n4  O | O | O | O | O | O | B-geo | O | O | O | O ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df = df.drop_duplicates()\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:09.038770Z","iopub.execute_input":"2025-02-05T17:27:09.038977Z","iopub.status.idle":"2025-02-05T17:27:09.580526Z","shell.execute_reply.started":"2025-02-05T17:27:09.038960Z","shell.execute_reply":"2025-02-05T17:27:09.579464Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                             sentence  \\\n0   Thousands | of | demonstrators | have | marche...   \n24  Families | of | soldiers | killed | in | the |...   \n54  They | marched | from | the | Houses | of | Pa...   \n68  Police | put | the | number | of | marchers | ...   \n83  The | protest | comes | on | the | eve | of | ...   \n\n                                          word_labels  \n0   O | O | O | O | O | O | B-geo | O | O | O | O ...  \n24  O | O | O | O | O | O | O | O | O | O | O | O ...  \n54  O | O | O | O | O | O | O | O | O | O | O | B-...  \n68  O | O | O | O | O | O | O | O | O | O | O | O ...  \n83  O | O | O | O | O | O | O | O | O | O | O | B-...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Families | of | soldiers | killed | in | the |...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>They | marched | from | the | Houses | of | Pa...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | B-...</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>Police | put | the | number | of | marchers | ...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>The | protest | comes | on | the | eve | of | ...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | B-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df = df.reset_index(drop=True)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:09.581627Z","iopub.execute_input":"2025-02-05T17:27:09.581935Z","iopub.status.idle":"2025-02-05T17:27:09.597053Z","shell.execute_reply.started":"2025-02-05T17:27:09.581901Z","shell.execute_reply":"2025-02-05T17:27:09.595455Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                            sentence  \\\n0  Thousands | of | demonstrators | have | marche...   \n1  Families | of | soldiers | killed | in | the |...   \n2  They | marched | from | the | Houses | of | Pa...   \n3  Police | put | the | number | of | marchers | ...   \n4  The | protest | comes | on | the | eve | of | ...   \n\n                                         word_labels  \n0  O | O | O | O | O | O | B-geo | O | O | O | O ...  \n1  O | O | O | O | O | O | O | O | O | O | O | O ...  \n2  O | O | O | O | O | O | O | O | O | O | O | B-...  \n3  O | O | O | O | O | O | O | O | O | O | O | O ...  \n4  O | O | O | O | O | O | O | O | O | O | O | B-...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thousands | of | demonstrators | have | marche...</td>\n      <td>O | O | O | O | O | O | B-geo | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Families | of | soldiers | killed | in | the |...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They | marched | from | the | Houses | of | Pa...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | B-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Police | put | the | number | of | marchers | ...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The | protest | comes | on | the | eve | of | ...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | B-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 4\nTEST_BATCH_SIZE = 4\nEPOCHS = 1\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:38:17.030647Z","iopub.execute_input":"2025-02-05T17:38:17.031167Z","iopub.status.idle":"2025-02-05T17:38:17.193868Z","shell.execute_reply.started":"2025-02-05T17:38:17.031131Z","shell.execute_reply":"2025-02-05T17:38:17.193146Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    tokenized_sentence = []\n    labels = []\n\n    sentence = sentence.strip()\n    text_labels = text_labels.strip()\n\n    for word, label in zip(sentence.split(' | '), text_labels.split(\" | \")):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer.tokenize(word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence.extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n        labels.extend([label] * len(tokenized_word))\n\n    return tokenized_sentence, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:09.805157Z","iopub.execute_input":"2025-02-05T17:27:09.805403Z","iopub.status.idle":"2025-02-05T17:27:09.810024Z","shell.execute_reply.started":"2025-02-05T17:27:09.805356Z","shell.execute_reply":"2025-02-05T17:27:09.809202Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"['0'] + [\"xwdw\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:27:09.810817Z","iopub.execute_input":"2025-02-05T17:27:09.811076Z","iopub.status.idle":"2025-02-05T17:27:09.823390Z","shell.execute_reply.started":"2025-02-05T17:27:09.811056Z","shell.execute_reply":"2025-02-05T17:27:09.822685Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['0', 'xwdw']"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, label2id):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.label2id = label2id\n        \n    def __getitem__(self, index):\n        sentence = self.data['sentence'].iloc[index]\n        word_labels = self.data['word_labels'].iloc[index]\n\n        # Tokenize and align labels\n        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n\n        # Add special tokens\n        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n        labels = [\"O\"] + labels + [\"O\"]  # 'O' for outside tokens\n\n        # Truncate if exceeding max length\n        if len(tokenized_sentence) > self.max_len:\n            tokenized_sentence = tokenized_sentence[:self.max_len - 1] + [\"[SEP]\"]\n            labels = labels[:self.max_len - 1] + [\"O\"]\n\n        # Pad sequences if needed\n        while len(tokenized_sentence) < self.max_len:\n            tokenized_sentence.append(\"[PAD]\")\n            labels.append(\"O\")\n\n        # Attention mask (1 for real tokens, 0 for padding)\n        attn_mask = [1 if token != \"[PAD]\" else 0 for token in tokenized_sentence]\n\n        # Convert tokens and labels to IDs\n        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n        label_ids = [self.label2id[label] for label in labels]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(attn_mask, dtype=torch.long),\n            'targets': torch.tensor(label_ids, dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:31:44.984185Z","iopub.execute_input":"2025-02-05T17:31:44.984571Z","iopub.status.idle":"2025-02-05T17:31:44.992156Z","shell.execute_reply.started":"2025-02-05T17:31:44.984540Z","shell.execute_reply":"2025-02-05T17:31:44.991193Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(tokenizer.convert_tokens_to_ids(\"[PAD]\"))\nprint(tokenizer.convert_tokens_to_ids(\"[SEP]\"))\nprint(tokenizer.convert_tokens_to_ids(\"[CLS]\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:31:43.718721Z","iopub.execute_input":"2025-02-05T19:31:43.719029Z","iopub.status.idle":"2025-02-05T19:31:43.723912Z","shell.execute_reply.started":"2025-02-05T19:31:43.719005Z","shell.execute_reply":"2025-02-05T19:31:43.723024Z"}},"outputs":[{"name":"stdout","text":"0\n102\n101\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:31:45.714596Z","iopub.execute_input":"2025-02-05T17:31:45.714920Z","iopub.status.idle":"2025-02-05T17:31:45.732576Z","shell.execute_reply.started":"2025-02-05T17:31:45.714896Z","shell.execute_reply":"2025-02-05T17:31:45.731858Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:31:46.616558Z","iopub.execute_input":"2025-02-05T17:31:46.616874Z","iopub.status.idle":"2025-02-05T17:31:46.626359Z","shell.execute_reply.started":"2025-02-05T17:31:46.616852Z","shell.execute_reply":"2025-02-05T17:31:46.625584Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                                sentence  \\\n0      Parliament | was | due | to | open | Monday | ...   \n1      Iraqi | legislators | have | been | grappling ...   \n2      The | singer | arrived | in | the | southern |...   \n3      The | director | of | the | Liberia | Electric...   \n4      Israel | has | assassinated | Hamas | founder ...   \n...                                                  ...   \n38083  \" | You | do | me | a | grave | injustice | , ...   \n38084  The | U.S. | State | Department | says | one |...   \n38085  The | committee | is | to | select | 2,000 | p...   \n38086  Those | groups | were | shut | down | by | U.S...   \n38087  Their | directors | say | they | want | to | s...   \n\n                                             word_labels  \n0      B-org | O | O | O | O | B-tim | O | O | O | O ...  \n1      B-gpe | O | O | O | O | O | O | O | O | O | O ...  \n2      O | O | O | O | O | O | B-gpe | O | O | B-tim ...  \n3      O | O | O | O | B-geo | I-geo | I-geo | O | B-...  \n4      B-geo | O | O | B-org | O | O | O | O | O | O ...  \n...                                                  ...  \n38083  O | O | O | O | O | O | O | O | O | O | O | O ...  \n38084  O | B-org | I-org | I-org | O | O | O | O | O ...  \n38085  O | O | O | O | O | O | O | O | O | B-org | I-...  \n38086  O | O | O | O | O | O | B-org | O | O | O | B-...  \n38087  O | O | O | O | O | O | O | O | O | O | O | O ...  \n\n[38088 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Parliament | was | due | to | open | Monday | ...</td>\n      <td>B-org | O | O | O | O | B-tim | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Iraqi | legislators | have | been | grappling ...</td>\n      <td>B-gpe | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The | singer | arrived | in | the | southern |...</td>\n      <td>O | O | O | O | O | O | B-gpe | O | O | B-tim ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The | director | of | the | Liberia | Electric...</td>\n      <td>O | O | O | O | B-geo | I-geo | I-geo | O | B-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Israel | has | assassinated | Hamas | founder ...</td>\n      <td>B-geo | O | O | B-org | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38083</th>\n      <td>\" | You | do | me | a | grave | injustice | , ...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>38084</th>\n      <td>The | U.S. | State | Department | says | one |...</td>\n      <td>O | B-org | I-org | I-org | O | O | O | O | O ...</td>\n    </tr>\n    <tr>\n      <th>38085</th>\n      <td>The | committee | is | to | select | 2,000 | p...</td>\n      <td>O | O | O | O | O | O | O | O | O | B-org | I-...</td>\n    </tr>\n    <tr>\n      <th>38086</th>\n      <td>Those | groups | were | shut | down | by | U.S...</td>\n      <td>O | O | O | O | O | O | B-org | O | O | O | B-...</td>\n    </tr>\n    <tr>\n      <th>38087</th>\n      <td>Their | directors | say | they | want | to | s...</td>\n      <td>O | O | O | O | O | O | O | O | O | O | O | O ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>38088 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"training_set = CustomDataset(train_df, tokenizer, MAX_LEN, label2id)\ntesting_set = CustomDataset(test_df, tokenizer, MAX_LEN, label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:31:13.460977Z","iopub.execute_input":"2025-02-05T17:31:13.461333Z","iopub.status.idle":"2025-02-05T17:31:13.465273Z","shell.execute_reply.started":"2025-02-05T17:31:13.461308Z","shell.execute_reply":"2025-02-05T17:31:13.464464Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"training_set[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:31:55.958109Z","iopub.execute_input":"2025-02-05T17:31:55.958447Z","iopub.status.idle":"2025-02-05T17:31:55.995464Z","shell.execute_reply.started":"2025-02-05T17:31:55.958419Z","shell.execute_reply":"2025-02-05T17:31:55.994571Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'ids': tensor([  101,  3323,  2001,  2349,  2000,  2330,  6928,  1010,  2021,  2008,\n          5219,  2001,  2404,  2125,  2004, 11895,  1005,  2009,  2229,  1010,\n         18883, 14560,  1998, 13970, 17811,  2699,  2000,  5993,  2006,  2040,\n          2097,  3710,  2004,  3539,  2704,  1012,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]),\n 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'targets': tensor([0, 5, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 5, 6,\n         6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0])}"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle = True, num_workers = 0)\ntesting_loader = DataLoader(testing_set, batch_size = TEST_BATCH_SIZE, shuffle = False, num_workers = 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:38:28.210933Z","iopub.execute_input":"2025-02-05T17:38:28.211235Z","iopub.status.idle":"2025-02-05T17:38:28.215611Z","shell.execute_reply.started":"2025-02-05T17:38:28.211214Z","shell.execute_reply":"2025-02-05T17:38:28.214750Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n                                                   num_labels=len(id2label),\n                                                   id2label=id2label,\n                                                   label2id=label2id)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:38:56.022779Z","iopub.execute_input":"2025-02-05T17:38:56.023115Z","iopub.status.idle":"2025-02-05T17:38:58.719739Z","shell.execute_reply.started":"2025-02-05T17:38:56.023090Z","shell.execute_reply":"2025-02-05T17:38:58.718911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c69993d648c74009a3e7992f30520ffc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=17, bias=True)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"loss = 0\ncount = 0\n\nwith torch.no_grad():  # Prevents computation graph buildup\n    for x in testing_set:\n        ids = x[\"ids\"].unsqueeze(0).to(device)\n        mask = x[\"mask\"].unsqueeze(0).to(device)\n        targets = x[\"targets\"].unsqueeze(0).to(device)\n\n        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n        loss += outputs.loss.item()  # Convert loss tensor to scalar\n\n        count += 1\n\n        # Free memory\n        del ids, mask, targets, outputs\n        torch.cuda.empty_cache()\n\nprint(loss / count if count > 0 else 0)  # Avoid division by zero","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:54:05.322754Z","iopub.execute_input":"2025-02-05T17:54:05.323081Z","iopub.status.idle":"2025-02-05T17:55:49.184647Z","shell.execute_reply.started":"2025-02-05T17:54:05.323057Z","shell.execute_reply":"2025-02-05T17:55:49.183741Z"}},"outputs":[{"name":"stdout","text":"2.8942694085607306\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"import math\n\nresult = -math.log(1/17)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:57:05.034048Z","iopub.execute_input":"2025-02-05T17:57:05.034390Z","iopub.status.idle":"2025-02-05T17:57:05.039175Z","shell.execute_reply.started":"2025-02-05T17:57:05.034346Z","shell.execute_reply":"2025-02-05T17:57:05.038448Z"}},"outputs":[{"name":"stdout","text":"2.833213344056216\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"ids = training_set[0][\"ids\"].unsqueeze(0)\nmask = training_set[0][\"mask\"].unsqueeze(0)\ntargets = training_set[0][\"targets\"].unsqueeze(0)\nids = ids.to(device)\nmask = mask.to(device)\ntargets = targets.to(device)\noutputs = model(input_ids=ids, attention_mask=mask, labels=targets)\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:59:35.057157Z","iopub.execute_input":"2025-02-05T17:59:35.057559Z","iopub.status.idle":"2025-02-05T17:59:35.090014Z","shell.execute_reply.started":"2025-02-05T17:59:35.057530Z","shell.execute_reply":"2025-02-05T17:59:35.089266Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"TokenClassifierOutput(loss=tensor(2.7202, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.1914, -0.5881, -0.1467,  ...,  0.1348, -0.3043, -0.0886],\n         [ 0.1020, -0.2817,  0.1577,  ...,  0.1842,  0.0858, -0.1644],\n         [-0.0664, -0.4145, -0.0142,  ..., -0.2610,  0.0591,  0.4420],\n         ...,\n         [ 0.0926, -0.1537, -0.2456,  ..., -0.2243, -0.1140, -0.1150],\n         [ 0.0819, -0.1092, -0.2511,  ..., -0.1300, -0.1716, -0.0267],\n         [ 0.0378, -0.1661, -0.1913,  ..., -0.2972, -0.1454, -0.1949]]],\n       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"outputs[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:00:08.372499Z","iopub.execute_input":"2025-02-05T18:00:08.372833Z","iopub.status.idle":"2025-02-05T18:00:08.379763Z","shell.execute_reply.started":"2025-02-05T18:00:08.372809Z","shell.execute_reply":"2025-02-05T18:00:08.378933Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"tensor(2.7202, device='cuda:0', grad_fn=<NllLossBackward0>)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"outputs[1].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:00:14.871710Z","iopub.execute_input":"2025-02-05T18:00:14.872058Z","iopub.status.idle":"2025-02-05T18:00:14.878183Z","shell.execute_reply.started":"2025-02-05T18:00:14.872027Z","shell.execute_reply":"2025-02-05T18:00:14.877356Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128, 17])"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"outputs[1][0].","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:02:41.855620Z","iopub.execute_input":"2025-02-05T18:02:41.855949Z","iopub.status.idle":"2025-02-05T18:02:41.861516Z","shell.execute_reply.started":"2025-02-05T18:02:41.855919Z","shell.execute_reply":"2025-02-05T18:02:41.860664Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"torch.Size([128, 17])"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:45:19.148331Z","iopub.execute_input":"2025-02-05T18:45:19.148728Z","iopub.status.idle":"2025-02-05T18:45:19.154086Z","shell.execute_reply.started":"2025-02-05T18:45:19.148694Z","shell.execute_reply":"2025-02-05T18:45:19.153061Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\n\ndef train(model, training_loader, optimizer, device, max_grad_norm=1.0):\n    model.train()  # Set model to training mode\n\n    total_loss, total_accuracy = 0, 0\n    num_steps, num_examples = 0, 0\n    all_preds, all_labels = [], []\n\n    for step, batch in enumerate(training_loader):\n        # Move data to device\n        input_ids = batch['ids'].to(device, dtype=torch.long)\n        attention_mask = batch['mask'].to(device, dtype=torch.long)\n        target_labels = batch['targets'].to(device, dtype=torch.long)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_labels)\n        loss, logits = outputs.loss, outputs.logits\n        total_loss += loss.item()\n\n        # Step-wise accuracy calculation\n        flat_labels = target_labels.view(-1)\n        active_logits = logits.view(-1, model.num_labels)\n        predictions = torch.argmax(active_logits, axis=1)\n        valid_accuracy = attention_mask.view(-1) == 1\n        masked_labels = torch.masked_select(flat_labels, valid_accuracy)\n        masked_preds = torch.masked_select(predictions, valid_accuracy)\n\n        all_labels.extend(masked_labels)\n        all_preds.extend(masked_preds)\n\n        batch_accuracy = accuracy_score(masked_labels.cpu().numpy(), masked_preds.cpu().numpy())\n        total_accuracy += batch_accuracy\n\n        # Backward pass with gradient clipping\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        optimizer.step()\n\n        num_steps += 1\n        num_examples += target_labels.size(0)\n\n        if step % 100 == 0:\n            print(f\"Training Loss per 100 steps: {total_loss / num_steps}\")\n\n    avg_loss = total_loss / num_steps\n    avg_accuracy = total_accuracy / num_steps\n    print(f\"Training Loss: {avg_loss}\")\n    print(f\"Training Accuracy: {avg_accuracy}\")\n\n    return all_labels, all_preds\n\ndef validate(model, validation_loader, device):\n    model.eval()  # Set model to evaluation mode\n\n    total_loss, total_accuracy = 0, 0\n    num_steps, num_examples = 0, 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for step, batch in enumerate(validation_loader):\n            # Move data to device\n            input_ids = batch['ids'].to(device, dtype=torch.long)\n            attention_mask = batch['mask'].to(device, dtype=torch.long)\n            target_labels = batch['targets'].to(device, dtype=torch.long)\n\n            # Forward pass\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_labels)\n            loss, logits = outputs.loss, outputs.logits\n            total_loss += loss.item()\n\n            # Step-wise accuracy calculation\n            flat_labels = target_labels.view(-1)\n            active_logits = logits.view(-1, model.num_labels)\n            predictions = torch.argmax(active_logits, axis=1)\n            valid_accuracy = attention_mask.view(-1) == 1\n            masked_labels = torch.masked_select(flat_labels, valid_accuracy)\n            masked_preds = torch.masked_select(predictions, valid_accuracy)\n\n            all_labels.extend(masked_labels)\n            all_preds.extend(masked_preds)\n\n            batch_accuracy = accuracy_score(masked_labels.cpu().numpy(), masked_preds.cpu().numpy())\n            total_accuracy += batch_accuracy\n\n            num_steps += 1\n            num_examples += target_labels.size(0)\n\n            if step % 100 == 0:\n                print(f\"Validation Loss per 100 steps: {total_loss / num_steps}\")\n\n    avg_loss = total_loss / num_steps\n    avg_accuracy = total_accuracy / num_steps\n    print(f\"Validation Loss: {avg_loss}\")\n    print(f\"Validation Accuracy: {avg_accuracy}\")\n\n    return all_labels, all_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:44:17.622389Z","iopub.execute_input":"2025-02-05T18:44:17.622787Z","iopub.status.idle":"2025-02-05T18:44:17.634706Z","shell.execute_reply.started":"2025-02-05T18:44:17.622761Z","shell.execute_reply":"2025-02-05T18:44:17.633780Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    train_labels, train_preds = train(model, training_loader, optimizer, device)\n    val_labels, val_preds = validate(model, testing_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:45:53.936807Z","iopub.execute_input":"2025-02-05T18:45:53.937327Z","iopub.status.idle":"2025-02-05T19:07:10.235388Z","shell.execute_reply.started":"2025-02-05T18:45:53.937286Z","shell.execute_reply":"2025-02-05T19:07:10.234595Z"}},"outputs":[{"name":"stdout","text":"Training epoch: 1\nTraining Loss per 100 steps: 2.8713955879211426\nTraining Loss per 100 steps: 0.4922046587608828\nTraining Loss per 100 steps: 0.3004158263219826\nTraining Loss per 100 steps: 0.22456399126108303\nTraining Loss per 100 steps: 0.18549804672357298\nTraining Loss per 100 steps: 0.16066191021256343\nTraining Loss per 100 steps: 0.1429838609886348\nTraining Loss per 100 steps: 0.12959711419779824\nTraining Loss per 100 steps: 0.11984057831560516\nTraining Loss per 100 steps: 0.11212057309156981\nTraining Loss per 100 steps: 0.10590815999043676\nTraining Loss per 100 steps: 0.09984094703817008\nTraining Loss per 100 steps: 0.09503537321309687\nTraining Loss per 100 steps: 0.09072772476164986\nTraining Loss per 100 steps: 0.08728892435396168\nTraining Loss per 100 steps: 0.0841294562155263\nTraining Loss per 100 steps: 0.08119479804035093\nTraining Loss per 100 steps: 0.07862273861625632\nTraining Loss per 100 steps: 0.07641862449830895\nTraining Loss per 100 steps: 0.07440255344970283\nTraining Loss per 100 steps: 0.07258600530704239\nTraining Loss per 100 steps: 0.07100283166793472\nTraining Loss per 100 steps: 0.06960276550417009\nTraining Loss per 100 steps: 0.06818616036003919\nTraining Loss per 100 steps: 0.06684275389678454\nTraining Loss per 100 steps: 0.06560506450868987\nTraining Loss per 100 steps: 0.06446437671133745\nTraining Loss per 100 steps: 0.06316957744659932\nTraining Loss per 100 steps: 0.06214368953629736\nTraining Loss per 100 steps: 0.06116791042705188\nTraining Loss per 100 steps: 0.060203599874235895\nTraining Loss per 100 steps: 0.05932139066751189\nTraining Loss per 100 steps: 0.0585236869441748\nTraining Loss per 100 steps: 0.05786087086843341\nTraining Loss per 100 steps: 0.05714314694918081\nTraining Loss per 100 steps: 0.056487938425958825\nTraining Loss per 100 steps: 0.05587942995414473\nTraining Loss per 100 steps: 0.05535923910361982\nTraining Loss per 100 steps: 0.054815704498530196\nTraining Loss per 100 steps: 0.05435929017272649\nTraining Loss per 100 steps: 0.05384042168435774\nTraining Loss per 100 steps: 0.0533720530780526\nTraining Loss per 100 steps: 0.05287659036694926\nTraining Loss per 100 steps: 0.05245072765096273\nTraining Loss per 100 steps: 0.05199706934780927\nTraining Loss per 100 steps: 0.05157079485071396\nTraining Loss per 100 steps: 0.051185616006790016\nTraining Loss per 100 steps: 0.05084730174841531\nTraining Loss per 100 steps: 0.05041023970242505\nTraining Loss per 100 steps: 0.04998612600400355\nTraining Loss per 100 steps: 0.049642925725715714\nTraining Loss per 100 steps: 0.04927459961411187\nTraining Loss per 100 steps: 0.048895943448519795\nTraining Loss per 100 steps: 0.04860754679788169\nTraining Loss per 100 steps: 0.04842343260232777\nTraining Loss per 100 steps: 0.04802926412613501\nTraining Loss per 100 steps: 0.04771488174002585\nTraining Loss per 100 steps: 0.04737454232105471\nTraining Loss per 100 steps: 0.04705680506448196\nTraining Loss per 100 steps: 0.046804008997713684\nTraining Loss per 100 steps: 0.04656515570351118\nTraining Loss per 100 steps: 0.046295908799788725\nTraining Loss per 100 steps: 0.046023163186243776\nTraining Loss per 100 steps: 0.04572135779067232\nTraining Loss per 100 steps: 0.04546634188963309\nTraining Loss per 100 steps: 0.045242620428312175\nTraining Loss per 100 steps: 0.045060483574394745\nTraining Loss per 100 steps: 0.044778480569501525\nTraining Loss per 100 steps: 0.04450998701808864\nTraining Loss per 100 steps: 0.04426589795317295\nTraining Loss per 100 steps: 0.044080972287185244\nTraining Loss per 100 steps: 0.04392576097362912\nTraining Loss per 100 steps: 0.04371347113531306\nTraining Loss per 100 steps: 0.043454461634078276\nTraining Loss per 100 steps: 0.04322093867159276\nTraining Loss per 100 steps: 0.04298304259100429\nTraining Loss per 100 steps: 0.042772210850057814\nTraining Loss per 100 steps: 0.042610876524746\nTraining Loss per 100 steps: 0.04248613549309155\nTraining Loss per 100 steps: 0.04231498639100243\nTraining Loss per 100 steps: 0.04214820937595928\nTraining Loss per 100 steps: 0.041997868644199264\nTraining Loss per 100 steps: 0.04180261796505608\nTraining Loss per 100 steps: 0.04165617622187397\nTraining Loss per 100 steps: 0.04151714481116956\nTraining Loss per 100 steps: 0.04138333948610016\nTraining Loss per 100 steps: 0.04123891354531056\nTraining Loss per 100 steps: 0.04112828961188639\nTraining Loss per 100 steps: 0.04097754698492326\nTraining Loss per 100 steps: 0.04086114248352042\nTraining Loss per 100 steps: 0.04073036981877649\nTraining Loss per 100 steps: 0.04060137121391819\nTraining Loss per 100 steps: 0.040456408073259155\nTraining Loss per 100 steps: 0.04032306099404415\nTraining Loss per 100 steps: 0.04016740611794257\nTraining Loss per 100 steps: 0.04003351506527858\nTraining Loss: 0.0400194308333615\nTraining Accuracy: 0.9501494843875321\nValidation Loss per 100 steps: 0.009070287458598614\nValidation Loss per 100 steps: 0.030068443311230823\nValidation Loss per 100 steps: 0.027412564303165313\nValidation Loss per 100 steps: 0.027482342854856925\nValidation Loss per 100 steps: 0.027260031020431372\nValidation Loss per 100 steps: 0.02761949284351663\nValidation Loss per 100 steps: 0.02773169452100202\nValidation Loss per 100 steps: 0.02738602243979314\nValidation Loss per 100 steps: 0.027071845308966475\nValidation Loss per 100 steps: 0.026747574303524008\nValidation Loss per 100 steps: 0.02669118024048523\nValidation Loss per 100 steps: 0.026764026226339277\nValidation Loss per 100 steps: 0.02657319406303679\nValidation Loss per 100 steps: 0.02647834274237715\nValidation Loss per 100 steps: 0.02648361959712628\nValidation Loss per 100 steps: 0.026549153976544507\nValidation Loss per 100 steps: 0.026428850952491402\nValidation Loss per 100 steps: 0.02638465006031545\nValidation Loss per 100 steps: 0.026356921121337976\nValidation Loss per 100 steps: 0.026356260562376364\nValidation Loss per 100 steps: 0.02628346155939891\nValidation Loss per 100 steps: 0.02618217723836916\nValidation Loss per 100 steps: 0.026262864172927933\nValidation Loss per 100 steps: 0.026412191689989494\nValidation Loss: 0.02633096954282186\nValidation Accuracy: 0.9602954282087363\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nval_labels_str = [id2label[label.item()] for label in val_labels]\nval_preds_str = [id2label[label.item()] for label in val_preds]\n\n# Print the classification report with string labels\nprint(classification_report(val_labels_str, val_preds_str))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:24:54.282210Z","iopub.execute_input":"2025-02-05T19:24:54.282628Z","iopub.status.idle":"2025-02-05T19:25:02.019988Z","shell.execute_reply.started":"2025-02-05T19:24:54.282599Z","shell.execute_reply":"2025-02-05T19:25:02.019015Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       B-art       0.50      0.02      0.03       113\n       B-eve       0.83      0.27      0.41        70\n       B-geo       0.87      0.86      0.86     11575\n       B-gpe       0.95      0.91      0.93      3437\n       B-nat       0.55      0.19      0.28        64\n       B-org       0.75      0.67      0.71      6823\n       B-per       0.81      0.87      0.84      5220\n       B-tim       0.93      0.83      0.88      4298\n       I-art       0.00      0.00      0.00        46\n       I-eve       1.00      0.09      0.16        46\n       I-geo       0.85      0.67      0.75      1772\n       I-gpe       1.00      0.55      0.71        53\n       I-nat       0.00      0.00      0.00        10\n       I-org       0.76      0.61      0.68      4237\n       I-per       0.81      0.95      0.88      6432\n       I-tim       0.88      0.72      0.79      1301\n           O       0.99      0.99      0.99    206918\n\n    accuracy                           0.96    252415\n   macro avg       0.73      0.54      0.58    252415\nweighted avg       0.96      0.96      0.96    252415\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":125},{"cell_type":"code","source":"print(tokenizer.convert_tokens_to_ids(\"[PAD]\"))\nprint(tokenizer.convert_tokens_to_ids(\"[SEP]\"))\nprint(tokenizer.convert_tokens_to_ids(\"[CLS]\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:31:56.530284Z","iopub.execute_input":"2025-02-05T19:31:56.530635Z","iopub.status.idle":"2025-02-05T19:31:56.535870Z","shell.execute_reply.started":"2025-02-05T19:31:56.530608Z","shell.execute_reply":"2025-02-05T19:31:56.535170Z"}},"outputs":[{"name":"stdout","text":"0\n102\n101\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"sentence = \"India has a capital called Mumbai. On wednesday, the president will give a presentation.India has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentationIndia has a capital called Mumbai. On wednesday, the president will give a presentation\"\n\ntokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:33:36.433526Z","iopub.execute_input":"2025-02-05T19:33:36.433899Z","iopub.status.idle":"2025-02-05T19:33:36.448652Z","shell.execute_reply.started":"2025-02-05T19:33:36.433872Z","shell.execute_reply":"2025-02-05T19:33:36.447641Z"}},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2634,  2038,  1037,  3007,  2170,  8955,  1012,  2006,  9317,\n          1010,  1996,  2343,  2097,  2507,  1037,  8312,  1012,  2634,  2038,\n          1037,  3007,  2170,  8955,  1012,  2006,  9317,  1010,  1996,  2343,\n          2097,  2507,  1037,  8312, 22254,  2401,  2038,  1037,  3007,  2170,\n          8955,  1012,  2006,  9317,  1010,  1996,  2343,  2097,  2507,  1037,\n          8312, 22254,  2401,  2038,  1037,  3007,  2170,  8955,  1012,  2006,\n          9317,  1010,  1996,  2343,  2097,  2507,  1037,  8312, 22254,  2401,\n          2038,  1037,  3007,  2170,  8955,  1012,  2006,  9317,  1010,  1996,\n          2343,  2097,  2507,  1037,  8312, 22254,  2401,  2038,  1037,  3007,\n          2170,  8955,  1012,  2006,  9317,  1010,  1996,  2343,  2097,  2507,\n          1037,  8312, 22254,  2401,  2038,  1037,  3007,  2170,  8955,  1012,\n          2006,  9317,  1010,  1996,  2343,  2097,  2507,  1037,  8312, 22254,\n          2401,  2038,  1037,  3007,  2170,  8955,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"inputs['input_ids'][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:41:53.907637Z","iopub.execute_input":"2025-02-05T19:41:53.907955Z","iopub.status.idle":"2025-02-05T19:41:53.913188Z","shell.execute_reply.started":"2025-02-05T19:41:53.907930Z","shell.execute_reply":"2025-02-05T19:41:53.912266Z"}},"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"torch.Size([128])"},"metadata":{}}],"execution_count":145},{"cell_type":"code","source":"sentence = \"India has a capital called Mumbai. On wednesday, the president will give a presentation\"\n\ninputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n\n# move to gpu\nids = inputs[\"input_ids\"].to(device)\nmask = inputs[\"attention_mask\"].to(device)\n\n# forward pass\noutputs = model(ids, mask)\nlogits = outputs[0]\n\nactive_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\nflattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\ntokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\ntoken_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\nwp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\nword_level_predictions = []\nfor pair in wp_preds:\n  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n    # skip prediction\n    continue\n  else:\n    word_level_predictions.append(pair[1])\n\n# we join tokens, if they are not special ones\nstr_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\nprint(str_rep)\nprint(word_level_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:47:54.056510Z","iopub.execute_input":"2025-02-05T19:47:54.056922Z","iopub.status.idle":"2025-02-05T19:47:54.084752Z","shell.execute_reply.started":"2025-02-05T19:47:54.056896Z","shell.execute_reply":"2025-02-05T19:47:54.083930Z"}},"outputs":[{"name":"stdout","text":"india has a capital called mumbai . on wednesday , the president will give a presentation\n['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}],"execution_count":150},{"cell_type":"code","source":"!sudo apt-get install git-lfs\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:56:45.671795Z","iopub.execute_input":"2025-02-05T19:56:45.672142Z","iopub.status.idle":"2025-02-05T19:56:45.689079Z","shell.execute_reply.started":"2025-02-05T19:56:45.672113Z","shell.execute_reply":"2025-02-05T19:56:45.688342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f0d530992a649dfb56af1479347ee96"}},"metadata":{}}],"execution_count":160},{"cell_type":"code","source":"model_name = \"bert-finetuned-named-entity-recognition\"\n\n# Upload tokenizer to the hub\ntokenizer.push_to_hub(\n    repo_id=\"ParitKansal/{}\".format(model_name),  # Correct repo_id format\n    commit_message=\"Add tokenizer\",\n    use_temp_dir=True,\n)\n\n# Upload model to the hub\nmodel.push_to_hub(\n    repo_id=\"ParitKansal/{}\".format(model_name),  # Correct repo_id format\n    commit_message=\"Add model\",\n    use_temp_dir=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T19:59:52.582177Z","iopub.execute_input":"2025-02-05T19:59:52.582525Z","iopub.status.idle":"2025-02-05T20:00:07.972520Z","shell.execute_reply.started":"2025-02-05T19:59:52.582496Z","shell.execute_reply":"2025-02-05T20:00:07.971822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e48279dc50442f8fba885ae8da943f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dccc0e602644b7983186f49c3c3b27e"}},"metadata":{}},{"execution_count":165,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ParitKansal/bert-finetuned-named-entity-recognition/commit/908040e09372f8ca5e696c462dd8478abe29f264', commit_message='Add model', commit_description='', oid='908040e09372f8ca5e696c462dd8478abe29f264', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ParitKansal/bert-finetuned-named-entity-recognition', endpoint='https://huggingface.co', repo_type='model', repo_id='ParitKansal/bert-finetuned-named-entity-recognition'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":165},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}